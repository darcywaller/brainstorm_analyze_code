{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c406dd0e",
   "metadata": {},
   "source": [
    "# 1) Extract beta event information from the entire sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9af9734",
   "metadata": {},
   "source": [
    "Using the spectral events toolbox, derive at the number, duration, peak power, and frequency span of beta events across the timepoints for all series. Further below I'll actually drill down into seeing what's going on with individual timepoints or making comparisons/predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e92632",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in packages\n",
    "%matplotlib widget\n",
    "\n",
    "import sys\n",
    "import os.path as op\n",
    "import os\n",
    "from glob import glob\n",
    "import pickle\n",
    "import statistics\n",
    "\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "from scipy.io import loadmat\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from mne.io import read_epochs_eeglab\n",
    "import mne\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "import pandas as pd\n",
    "\n",
    "# set path to SpectralEvents if necessary\n",
    "sys.path.append('/gpfs/home/ddiesbur/brainstorm-ws/spectral-events')\n",
    "import spectralevents as se"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd32c236",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in the data\n",
    "t1_fname = []\n",
    "t2_fname = []\n",
    "t1_val_fname = []\n",
    "t2_val_fname = []\n",
    "# iterating across and within series to get lists of our files.\n",
    "# later on we'll check the subject ID in file names and series number\n",
    "# to make sure we're comparing times for the same subject within the relevant series\n",
    "# NOTE THAT BEFORE I RAN THIS NOTEBOOK I USED THE MATLAB FILE applyCSD.m TO ADD A PREPROCESSING STEP\n",
    "# IN WHICH I APPLIED A CURRENT SOURCE DENSITY TRANSFORM TO MINIMIZE VOLUME CONDUCTION \n",
    "# training sets\n",
    "data_dir = '/gpfs/home/ddiesbur/scratch/CSD/'\n",
    "# get T1s\n",
    "for file_idx in os.listdir(data_dir):\n",
    "    if (\"Pre_Session1\" in file_idx) & (\".set\" in file_idx):\n",
    "        t1_fname.append(op.join(data_dir,file_idx))\n",
    "# get T2s    \n",
    "for file_idx in os.listdir(data_dir):\n",
    "    if (\"Post_Session1\" in file_idx) & (\".set\" in file_idx):\n",
    "        t2_fname.append(op.join(data_dir,file_idx))\n",
    "        \n",
    "# testing/validation sets        \n",
    "data_val_dir = '/gpfs/home/ddiesbur/scratch/CSD_val/'\n",
    "# get T1s\n",
    "for file_idx in os.listdir(data_val_dir):\n",
    "    if (\"Pre_Session1\" in file_idx) & (\".set\" in file_idx):\n",
    "        t1_val_fname.append(op.join(data_val_dir,file_idx))\n",
    "# get T2s    \n",
    "for file_idx in os.listdir(data_val_dir):\n",
    "    if (\"Post_Session1\" in file_idx) & (\".set\" in file_idx):\n",
    "        t2_val_fname.append(op.join(data_val_dir,file_idx))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac7490e",
   "metadata": {},
   "source": [
    "Extract beta evs for T1\n",
    "(Note that there are commented-out lines to toggle for whether I'm getting evs from the training or the validation sets.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3119720b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# settings for extraction\n",
    "n_times = 2000\n",
    "samp_freq = 500\n",
    "freqs = list(range(2, 45 + 1))  # fequency values (Hz) over which to calculate TFR\n",
    "times = np.arange(n_times) / samp_freq  # seconds\n",
    "event_band = [15, 29]  # beta band (Hz)\n",
    "thresh_FOM = 6.0  # factor-of-the-median threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15117e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get evs from T1 sets\n",
    "# For the sake of saving memory and not overwhelming my data directory, I'll be running each dataset through TFR \n",
    "# conversion and spectral event detection right away, deleting the full data when I'm done with it.\n",
    "# Spectral event info is saved in scratch with pickle.\n",
    "\n",
    "#t1_save_dir = '/gpfs/home/ddiesbur/scratch/spectral_events/T1/'\n",
    "t1_save_dir = '/gpfs/home/ddiesbur/scratch/spectral_events/T1_val/'\n",
    "\n",
    "for subj_idx, subj_file in enumerate(t1_val_fname):\n",
    "#for subj_idx, subj_file in enumerate(t1_fname):\n",
    "    # load eeg data\n",
    "    subj_ID = subj_file[-46:-42] # subject ID, they're not in order in the list, but we'll\n",
    "    # just grab this and the series ID so the new filename has that info for later\n",
    "    series_ID = subj_file[-22:-21]\n",
    "    epochs_t1 = read_epochs_eeglab(subj_file)\n",
    "    spec_events_dict = {}\n",
    "\n",
    "    # iterate channels\n",
    "    for chan_idx,chan_name in enumerate(epochs_t1.ch_names):\n",
    "\n",
    "        data_t1 = epochs_t1.get_data(chan_idx).squeeze()  # epoch x time for the channel of loop iteration\n",
    "\n",
    "        # TFR conversion\n",
    "        tfrs_t1 = se.tfr(data_t1, freqs, samp_freq)\n",
    "\n",
    "        # Get spectral events\n",
    "        spec_events = se.find_events(tfr=tfrs_t1, times=times, freqs=freqs,\n",
    "                                     event_band=event_band, threshold_FOM=thresh_FOM)\n",
    "        # put in dictionary with all other chans, so that evs can be indexed with chan name\n",
    "        spec_events_dict[chan_name] = spec_events\n",
    "\n",
    "        del spec_events, data_t1, tfrs_t1\n",
    "        \n",
    "    # make filename and pickle\n",
    "    save_file = op.join(t1_save_dir,'Betaevs_'+subj_ID+'_S'+series_ID+'_T1.pkl')\n",
    "    with open(save_file, 'wb') as file:\n",
    "        # A new file will be created\n",
    "        pickle.dump(spec_events_dict, file)\n",
    "\n",
    "del epochs_t1 # save memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3187500",
   "metadata": {},
   "source": [
    "Extract beta evs for T2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6250d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get evs from T2 sets\n",
    "#t2_save_dir = '/gpfs/home/ddiesbur/scratch/spectral_events/T2/'\n",
    "t2_save_dir = '/gpfs/home/ddiesbur/scratch/spectral_events/T2_val/'\n",
    "\n",
    "for subj_idx, subj_file in enumerate(t2_val_fname):\n",
    "#for subj_idx, subj_file in enumerate(t2_fname):\n",
    "    # load eeg data\n",
    "    subj_ID = subj_file[-47:-43] # subject ID, they're not in order in the list\n",
    "    series_ID = subj_file[-22:-21]\n",
    "    epochs_t2 = read_epochs_eeglab(subj_file)\n",
    "    spec_events_dict = {}\n",
    "\n",
    "    # iterate channels\n",
    "    for chan_idx,chan_name in enumerate(epochs_t2.ch_names):\n",
    "\n",
    "        data_t2 = epochs_t2.get_data(chan_idx).squeeze()  # epoch x time for the channel of loop iteration\n",
    "\n",
    "        # TFR conversion\n",
    "        tfrs_t2 = se.tfr(data_t2, freqs, samp_freq)\n",
    "\n",
    "        # Get spectral events\n",
    "        spec_events = se.find_events(tfr=tfrs_t2, times=times, freqs=freqs,\n",
    "                                     event_band=event_band, threshold_FOM=thresh_FOM)\n",
    "        # put in dictionary with all other chans\n",
    "        spec_events_dict[chan_name] = spec_events\n",
    "\n",
    "        del spec_events, data_t2, tfrs_t2\n",
    "        \n",
    "    # make filename and pickle\n",
    "    save_file = op.join(t2_save_dir,'Betaevs_'+subj_ID+'_S'+series_ID+'_T2.pkl')\n",
    "    with open(save_file, 'wb') as file:\n",
    "        # A new file will be created\n",
    "        pickle.dump(spec_events_dict, file)\n",
    "\n",
    "del epochs_t2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "705e3405",
   "metadata": {},
   "source": [
    "# 2) Get information about beta events, visualize, and predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b754e686",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ok, now that we've made new pickle files that hold our dictionaries of beta ev\n",
    "# characteristics, set up lists of those files for each time condition, which\n",
    "# we'll iterate through to extract important info about the beta evs.\n",
    "evsets_t1 = []\n",
    "evsets_t2 = []\n",
    "evsets_val_t1 = []\n",
    "evsets_val_t2 = []\n",
    "\n",
    "evs_t1_dir = '/gpfs/home/ddiesbur/scratch/spectral_events/T1/'\n",
    "evs_t2_dir = '/gpfs/home/ddiesbur/scratch/spectral_events/T2/'\n",
    "evs_t1_val_dir = '/gpfs/home/ddiesbur/scratch/spectral_events/T1_val/'\n",
    "evs_t2_val_dir = '/gpfs/home/ddiesbur/scratch/spectral_events/T2_val/'\n",
    "\n",
    "# training sets\n",
    "for file_idx in os.listdir(evs_t1_dir):\n",
    "    if file_idx.endswith(\".pkl\"):\n",
    "        evsets_t1.append(op.join(evs_t1_dir,file_idx))\n",
    "for file_idx in os.listdir(evs_t2_dir):\n",
    "    if file_idx.endswith(\".pkl\"):\n",
    "        evsets_t2.append(op.join(evs_t2_dir,file_idx))\n",
    "        \n",
    "# validation sets\n",
    "for file_idx in os.listdir(evs_t1_val_dir):\n",
    "    if file_idx.endswith(\".pkl\"):\n",
    "        evsets_val_t1.append(op.join(evs_t1_val_dir,file_idx))\n",
    "for file_idx in os.listdir(evs_t2_val_dir):\n",
    "    if file_idx.endswith(\".pkl\"):\n",
    "        evsets_val_t2.append(op.join(evs_t2_val_dir,file_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3855b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# necessary functions - each takes a dictionary of beta ev characteristics from one channel's worth of\n",
    "# data as input.\n",
    "def avg_event_count(subj_spec_events):\n",
    "\n",
    "    # initialize the event count to 0\n",
    "    event_count = []\n",
    "\n",
    "    # iterate over trials and add event counts depending on trial type\n",
    "    for trial_idx, trial_events in enumerate(subj_spec_events):\n",
    "        event_count.append(len(trial_events)) # iterating over trials (epochs), make \n",
    "        # list of how many beta evs in the trial (per 4s)\n",
    "        \n",
    "    return statistics.mean(event_count) # take avg, which gives us rate of beta evs per 4s epoch\n",
    "\n",
    "def avg_event_peakfreq(subj_spec_events):\n",
    "\n",
    "    # initialize the event count to 0\n",
    "    event_freq = []\n",
    "\n",
    "    # iterate over trials and add event counts depending on trial type\n",
    "    for trial_idx in enumerate(subj_spec_events):\n",
    "        for event_idx in enumerate(subj_spec_events[int(trial_idx[0])]):\n",
    "            event_freq.append(subj_spec_events[int(trial_idx[0])][int(event_idx[0])]['Peak Frequency'])\n",
    "            # iterating over trials and beta evs, grab peak frequency of each ev observed\n",
    "            \n",
    "    return statistics.mean(event_freq) # take avg, which gives us avg peak freq\n",
    "\n",
    "def avg_event_dur(subj_spec_events):\n",
    "\n",
    "    # initialize the event count to 0\n",
    "    event_dur = []\n",
    "\n",
    "    # iterate over trials and add event counts depending on trial type\n",
    "    for trial_idx in enumerate(subj_spec_events):\n",
    "        for event_idx in enumerate(subj_spec_events[int(trial_idx[0])]):\n",
    "            event_dur.append(subj_spec_events[int(trial_idx[0])][int(event_idx[0])]['Event Duration'])\n",
    "            # iterating over trials and beta evs, grab duration of each ev observed\n",
    "\n",
    "    return statistics.mean(event_dur) # take avg, which gives us avg ev duration\n",
    "\n",
    "def avg_event_pow(subj_spec_events):\n",
    "\n",
    "    # initialize the event count to 0\n",
    "    event_pow = []\n",
    "\n",
    "    # iterate over trials and add event counts depending on trial type\n",
    "    for trial_idx in enumerate(subj_spec_events):\n",
    "        for event_idx in enumerate(subj_spec_events[int(trial_idx[0])]):\n",
    "            event_pow.append(subj_spec_events[int(trial_idx[0])][int(event_idx[0])]['Normalized Peak Power'])\n",
    "            # iterating over trials and beta evs, grab normed power of each ev observed\n",
    "\n",
    "    return statistics.mean(event_pow) # take avg, which gives us avg ev power"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7132183a",
   "metadata": {},
   "source": [
    "Get information about differences between responders, non responders at baseline (T1) in the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f579c0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grab the clinical info we're going to need for our entire dataset\n",
    "import csv\n",
    "fields = []\n",
    "rows = []\n",
    "filename = '/gpfs/data/brainstorm-ws/data/TRAINING/TRAINING_Demographics and Clinical Outcomes_All Series.csv'\n",
    "row_subs = []\n",
    "\n",
    "with open(filename, 'r') as csvfile:\n",
    "    # creating a csv reader object\n",
    "    csvreader = csv.reader(csvfile)\n",
    "     \n",
    "    # extracting field names through first row\n",
    "    fields = next(csvreader)\n",
    " \n",
    "    # extracting each data row one by one\n",
    "    for row in csvreader:\n",
    "        rows.append(row)\n",
    "        \n",
    "# for ease of later search pad sub nums with zeros\n",
    "for isub in range(len(rows)):\n",
    "    row_subs.append(rows[isub][0].zfill(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3586e29d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# preallocate\n",
    "ev_count_t1 = np.empty((0,62))\n",
    "ev_freq_t1 = np.empty((0,62))\n",
    "ev_dur_t1 = np.empty((0,62))\n",
    "ev_pow_t1 = np.empty((0,62))\n",
    "resp = []\n",
    "noresp = []\n",
    "resp_or_no = []\n",
    "field2get = fields.index('LastRespIDSSR')\n",
    "epochs = read_epochs_eeglab(t1_fname[0]) # grab chan info from one of the eeg sets\n",
    "pos = epochs.info\n",
    "\n",
    "# cycle through subjects\n",
    "for subj_idx, subj_file in enumerate(evsets_t1):\n",
    "    # load evs data\n",
    "    subj_ID = subj_file[-14:-10] # subject ID, they're not in order in the list\n",
    "    series_ID = subj_file[-8:-7]\n",
    "    \n",
    "    # mark whether this sub a responder or no\n",
    "    this_sub = row_subs.index(subj_ID)\n",
    "    if int(rows[this_sub][field2get])==1:\n",
    "        resp.append(True)\n",
    "        noresp.append(False)\n",
    "    elif int(rows[this_sub][field2get])==0:\n",
    "        resp.append(False)\n",
    "        noresp.append(True)\n",
    "    resp_or_no.append(int(rows[this_sub][field2get]))\n",
    "    t1_evs_dict = []\n",
    "    \n",
    "    # load pickles\n",
    "    load_file = open(subj_file,'rb')\n",
    "    t1_evs_dict = pickle.load(load_file)\n",
    " \n",
    "    # peak freq, num, duration, amplitude\n",
    "    # make empty for each new subject\n",
    "    ev_freq_chans = []\n",
    "    ev_count_chans = []\n",
    "    ev_dur_chans = []\n",
    "    ev_pow_chans = []\n",
    "\n",
    "    for chan_name in t1_evs_dict.keys():\n",
    "        # iterate through channels to get avg beta characteristics at each (avg'ed over all 4s epochs)\n",
    "        ev_freq_chans.append(avg_event_peakfreq(t1_evs_dict[chan_name]))\n",
    "        ev_count_chans.append(avg_event_count(t1_evs_dict[chan_name]))\n",
    "        ev_dur_chans.append(avg_event_dur(t1_evs_dict[chan_name]))\n",
    "        ev_pow_chans.append(avg_event_pow(t1_evs_dict[chan_name]))\n",
    "    \n",
    "    # append avg for each subject as we iterate through them\n",
    "    ev_freq_t1 = np.append(ev_freq_t1, [ev_freq_chans], axis=0)\n",
    "    ev_count_t1 = np.append(ev_count_t1, [ev_count_chans], axis=0)\n",
    "    ev_dur_t1 = np.append(ev_dur_t1, [ev_dur_chans], axis=0)\n",
    "    ev_pow_t1 = np.append(ev_pow_t1, [ev_pow_chans], axis=0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbfb6698",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot results - freq\n",
    "plt.close('all')\n",
    "freq_data = np.mean(ev_freq_t1[resp],axis=0)-np.mean(ev_freq_t1[noresp],axis=0)\n",
    "fig1 = mne.viz.plot_topomap(freq_data, pos,names=list(t1_evs_dict.keys()),show_names=True)\n",
    "\n",
    "# stats\n",
    "tvalue, pvalue = stats.ttest_ind(np.mean(ev_freq_t1[resp],axis=1),np.mean(ev_freq_t1[noresp],axis=1))\n",
    "print('Overall differences in global avg beta ev freq: = '+str(tvalue)+', p = '+str(pvalue))\n",
    "\n",
    "# t-tests w/ FDR\n",
    "T = []\n",
    "P = []\n",
    "for chan_idx in range(len(t1_evs_dict.keys())):\n",
    "    tvalue, pvalue = stats.ttest_ind(ev_freq_t1[resp,chan_idx],ev_freq_t1[noresp,chan_idx])\n",
    "    T.append(tvalue)\n",
    "    P.append(pvalue)\n",
    "_,corrP = mne.stats.fdr_correction(P, alpha=0.05, method='indep')    \n",
    "\n",
    "sig_channels = [i for i in range(len(P)) if P[i] <= 0.05]\n",
    "print('There are significant differences in beta ev frequency between responders and non-responders at channels '+str(sig_channels)+'.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3081707d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot results - counts\n",
    "plt.close('all')\n",
    "count_data = np.mean(ev_count_t1[resp],axis=0)-np.mean(ev_count_t1[noresp],axis=0)\n",
    "fig2 = mne.viz.plot_topomap(count_data, pos,names=list(t1_evs_dict.keys()),show_names=True)\n",
    "\n",
    "# stats\n",
    "tvalue, pvalue = stats.ttest_ind(np.mean(ev_count_t1[resp],axis=1),np.mean(ev_count_t1[noresp],axis=1))\n",
    "print('Overall diff in global evg beta ev count: F = '+str(tvalue)+', p = '+str(pvalue))\n",
    "\n",
    "# t-tests w/ FDR\n",
    "T = []\n",
    "P = []\n",
    "for chan_idx in range(len(t1_evs_dict.keys())):\n",
    "    tvalue, pvalue = stats.ttest_ind(ev_count_t1[resp,chan_idx],ev_count_t1[noresp,chan_idx])\n",
    "    T.append(tvalue)\n",
    "    P.append(pvalue)\n",
    "_,corrP = mne.stats.fdr_correction(P, alpha=0.05, method='indep')    \n",
    "sig_channels = [i for i in range(len(P)) if P[i] <= 0.05]\n",
    "print('There are significant differences in beta ev count between responders and non-responders at channels '+str(sig_channels)+'.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f44fca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot results - duration\n",
    "plt.close('all')\n",
    "dur_data = np.mean(ev_dur_t1[resp],axis=0)-np.mean(ev_dur_t1[noresp],axis=0)\n",
    "fig3 = mne.viz.plot_topomap(dur_data, pos,names=list(t1_evs_dict.keys()),show_names=True);\n",
    "\n",
    "# stats \n",
    "ftvalue, pvalue = stats.ttest_ind(np.mean(ev_dur_t1[resp],axis=1),np.mean(ev_dur_t1[noresp],axis=1))\n",
    "print('Overall diff in global avg eta ev duration: T = '+str(tvalue)+', p = '+str(pvalue))\n",
    "\n",
    "# t-tests w/ FDR\n",
    "T = []\n",
    "P = []\n",
    "for chan_idx in range(len(t1_evs_dict.keys())):\n",
    "    tvalue, pvalue = stats.ttest_ind(ev_dur_t1[resp,chan_idx],ev_dur_t1[noresp,chan_idx])\n",
    "    T.append(tvalue)\n",
    "    P.append(pvalue)\n",
    "_,corrP = mne.stats.fdr_correction(P, alpha=0.05, method='indep')    \n",
    "sig_channels = [i for i in range(len(P)) if P[i] <= 0.05]\n",
    "print('There are significant differences in beta ev duration between responders and non-responders at channels '+str(sig_channels)+'.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f81b1a2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plot results - power\n",
    "plt.close('all')\n",
    "pow_data = np.mean(ev_pow_t1[resp],axis=0)-np.mean(ev_pow_t1[noresp],axis=0)\n",
    "fig4,ax4 = mne.viz.plot_topomap(pow_data, pos,names=list(t1_evs_dict.keys()),show_names=True);\n",
    "\n",
    "# stats \n",
    "tvalue, pvalue = stats.ttest_ind(np.mean(ev_pow_t1[resp],axis=1),np.mean(ev_pow_t1[noresp],axis=1))\n",
    "print('Overall diff in global avg beta ev power: T = '+str(tvalue)+', p = '+str(pvalue))\n",
    "\n",
    "# t-tests w/ FDR\n",
    "T = []\n",
    "P = []\n",
    "for chan_idx in range(len(t1_evs_dict.keys())):\n",
    "    tvalue, pvalue = stats.ttest_ind(ev_pow_t1[resp,chan_idx],ev_pow_t1[noresp,chan_idx])\n",
    "    T.append(tvalue)\n",
    "    P.append(pvalue)\n",
    "_,corrP = mne.stats.fdr_correction(P, alpha=0.05, method='indep')\n",
    "sig_channels = [i for i in range(len(P)) if P[i] <= 0.05]\n",
    "print('There are significant differences in beta ev power between responders and non-responders at channels '+str(sig_channels)+'.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db7eb83",
   "metadata": {},
   "outputs": [],
   "source": [
    "chan_idx = list(t1_evs_dict.keys()).index('FC3')\n",
    "data = [ev_count_t1[resp,chan_idx], ev_count_t1[noresp,chan_idx]]\n",
    "fig1, ax1 = plt.subplots()\n",
    "ax1.set_title('Channel-wise diff for resp/non-resp')\n",
    "ax1.boxplot(data);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba2282aa",
   "metadata": {},
   "source": [
    "Get information about beta ev differences from T1 to T2 at all channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3537ecae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preallocate\n",
    "ev_count_t1t2 = np.empty((0,62))\n",
    "ev_freq_t1t2 = np.empty((0,62))\n",
    "ev_dur_t1t2 = np.empty((0,62))\n",
    "ev_pow_t1t2 = np.empty((0,62))\n",
    "resp = []\n",
    "noresp = []\n",
    "resp_or_no = []\n",
    "field2get = fields.index('LastRespIDSSR')\n",
    "epochs = read_epochs_eeglab(t1_fname[0])\n",
    "pos = epochs.info\n",
    "\n",
    "# cycle through subjects\n",
    "for subj_idx, subj_file in enumerate(evsets_t2):\n",
    "    # load evs data\n",
    "    subj_ID = subj_file[-14:-10] # subject ID, they're not in order in the list\n",
    "    series_ID = subj_file[-8:-7]\n",
    "    \n",
    "    # mark whether this sub a responder or no\n",
    "    this_sub = row_subs.index(subj_ID)\n",
    "    if int(rows[this_sub][field2get])==1:\n",
    "        resp.append(True)\n",
    "        noresp.append(False)\n",
    "    elif int(rows[this_sub][field2get])==0:\n",
    "        resp.append(False)\n",
    "        noresp.append(True)\n",
    "    resp_or_no.append(int(rows[this_sub][field2get]))\n",
    "    \n",
    "    t2_evs_dict = []\n",
    "    t1_evs_dict = []\n",
    "    \n",
    "    # load pickles\n",
    "    load_file = open(subj_file,'rb')\n",
    "    t2_evs_dict = pickle.load(load_file)\n",
    "    # find which sub coresponds to t2\n",
    "    index = [idx for idx, s in enumerate(evsets_t1) if str(subj_ID+'_S'+series_ID) in s]\n",
    "    load_file = open(evsets_t1[index[0]],'rb')\n",
    "    t1_evs_dict = pickle.load(load_file)\n",
    "    \n",
    "    # peak freq, num, duration, amplitude\n",
    "    ev_freq_chans = []\n",
    "    ev_count_chans = []\n",
    "    ev_dur_chans = []\n",
    "    ev_pow_chans = []\n",
    "\n",
    "    for chan_name in t1_evs_dict.keys():\n",
    "        ev_freq_chans.append(100*(avg_event_peakfreq(t2_evs_dict[chan_name])-avg_event_peakfreq(t1_evs_dict[chan_name]))/avg_event_peakfreq(t1_evs_dict[chan_name]))\n",
    "        ev_count_chans.append(100*(avg_event_count(t2_evs_dict[chan_name])-avg_event_count(t1_evs_dict[chan_name]))/avg_event_count(t1_evs_dict[chan_name]))\n",
    "        ev_dur_chans.append(100*(avg_event_dur(t2_evs_dict[chan_name])-avg_event_dur(t1_evs_dict[chan_name]))/avg_event_dur(t1_evs_dict[chan_name]))\n",
    "        ev_pow_chans.append(100*(avg_event_pow(t2_evs_dict[chan_name])-avg_event_pow(t1_evs_dict[chan_name]))/avg_event_pow(t1_evs_dict[chan_name]))\n",
    "        \n",
    "    ev_freq_t1t2 = np.append(ev_freq_t1t2, [ev_freq_chans], axis=0)\n",
    "    ev_count_t1t2 = np.append(ev_count_t1t2, [ev_count_chans], axis=0)\n",
    "    ev_dur_t1t2 = np.append(ev_dur_t1t2, [ev_dur_chans], axis=0)\n",
    "    ev_pow_t1t2 = np.append(ev_pow_t1t2, [ev_pow_chans], axis=0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9c2104",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot results - freq\n",
    "plt.close('all')\n",
    "freq_data = np.mean(ev_freq_t1t2[resp],axis=0)-np.mean(ev_freq_t1t2[noresp],axis=0)\n",
    "fig1 = mne.viz.plot_topomap(freq_data, pos,names=list(t1_evs_dict.keys()),show_names=True)\n",
    "\n",
    "# stats\n",
    "tvalue, pvalue = stats.ttest_ind(np.mean(ev_freq_t1t2[resp],axis=1),np.mean(ev_freq_t1t2[noresp],axis=1))\n",
    "print('Overall diff in global avg beta ev freq T2-T1: T = '+str(tvalue)+', p = '+str(pvalue))\n",
    "\n",
    "# t-tests w/ FDR\n",
    "T = []\n",
    "P = []\n",
    "for chan_idx in range(len(t1_evs_dict.keys())):\n",
    "    tvalue, pvalue = stats.ttest_ind(ev_freq_t1t2[resp,chan_idx],ev_freq_t1t2[noresp,chan_idx])\n",
    "    T.append(tvalue)\n",
    "    P.append(pvalue)\n",
    "_,corrP = mne.stats.fdr_correction(P, alpha=0.05, method='indep')    \n",
    "\n",
    "sig_channels = [i for i in range(len(P)) if P[i] <= 0.05]\n",
    "print('There are significant differences in beta ev frequency diff from T2-T1 between responders and non-responders at channels '+str(sig_channels)+'.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd370a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot results - counts\n",
    "plt.close('all')\n",
    "count_data = np.mean(ev_count_t1t2[resp],axis=0)-np.mean(ev_count_t1t2[noresp],axis=0)\n",
    "fig2 = mne.viz.plot_topomap(count_data, pos,names=list(t1_evs_dict.keys()),show_names=True)\n",
    "\n",
    "# stats\n",
    "tvalue, pvalue = stats.ttest_ind(np.mean(ev_count_t1t2[resp],axis=1),np.mean(ev_count_t1t2[noresp],axis=1))\n",
    "print('Overall diff in global avg beta ev count T2-T1: T = '+str(tvalue)+', p = '+str(pvalue))\n",
    "\n",
    "# t-tests w/ FDR\n",
    "T = []\n",
    "P = []\n",
    "for chan_idx in range(len(t1_evs_dict.keys())):\n",
    "    tvalue, pvalue = stats.ttest_ind(ev_count_t1t2[resp,chan_idx],ev_count_t1t2[noresp,chan_idx])\n",
    "    T.append(tvalue)\n",
    "    P.append(pvalue)\n",
    "_,corrP = mne.stats.fdr_correction(P, alpha=0.05, method='indep')    \n",
    "sig_channels = [i for i in range(len(P)) if P[i] <= 0.05]\n",
    "print('There are significant differences in beta ev count from T2-T1 between responders and non-responders at channels '+str(sig_channels)+'.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "889be126",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot results - duration\n",
    "plt.close('all')\n",
    "dur_data = np.mean(ev_dur_t1t2[resp],axis=0)-np.mean(ev_dur_t1t2[noresp],axis=0)\n",
    "fig3 = mne.viz.plot_topomap(dur_data, pos,names=list(t1_evs_dict.keys()),show_names=True);\n",
    "\n",
    "# stats \n",
    "tvalue, pvalue = stats.ttest_ind(np.mean(ev_dur_t1t2[resp],axis=1),np.mean(ev_dur_t1t2[noresp],axis=1))\n",
    "print('Overall diff in global avg beta ev duration T2-T1: T = '+str(tvalue)+', p = '+str(pvalue))\n",
    "\n",
    "# t-tests w/ FDR\n",
    "T = []\n",
    "P = []\n",
    "for chan_idx in range(len(t1_evs_dict.keys())):\n",
    "    tvalue, pvalue = stats.ttest_ind(ev_dur_t1t2[resp,chan_idx],ev_dur_t1t2[noresp,chan_idx])\n",
    "    T.append(tvalue)\n",
    "    P.append(pvalue)\n",
    "_,corrP = mne.stats.fdr_correction(P, alpha=0.05, method='indep')    \n",
    "sig_channels = [i for i in range(len(P)) if P[i] <= 0.05]\n",
    "print('There are significant differences in beta ev duration from T2-T1 between responders and non-responders at channels '+str(sig_channels)+'.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12e8d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot results - power\n",
    "plt.close('all')\n",
    "pow_data = np.mean(ev_pow_t1t2[resp],axis=0)-np.mean(ev_pow_t1t2[noresp],axis=0)\n",
    "fig4,ax4 = mne.viz.plot_topomap(pow_data, pos,names=list(t1_evs_dict.keys()),show_names=True);\n",
    "\n",
    "# stats \n",
    "tvalue, pvalue = stats.ttest_ind(np.mean(ev_pow_t1t2[resp],axis=1),np.mean(ev_pow_t1t2[noresp],axis=1))\n",
    "print('Overall diff in global avg beta ev power T2-T1: T = '+str(tvalue)+', p = '+str(pvalue))\n",
    "\n",
    "# t-tests w/ FDR\n",
    "T = []\n",
    "P = []\n",
    "for chan_idx in range(len(t1_evs_dict.keys())):\n",
    "    tvalue, pvalue = stats.ttest_ind(ev_pow_t1t2[resp,chan_idx],ev_pow_t1t2[noresp,chan_idx])\n",
    "    T.append(tvalue)\n",
    "    P.append(pvalue)\n",
    "_,corrP = mne.stats.fdr_correction(P, alpha=0.05, method='indep')\n",
    "sig_channels = [i for i in range(len(P)) if P[i] <= 0.05]\n",
    "print(P)\n",
    "print('There are significant differences in beta ev power from T2-T1 between responders and non-responders at channels '+str(sig_channels)+'.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65292b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "chan_idx = list(t1_evs_dict.keys()).index('C2')\n",
    "data = [ev_dur_t1t2[resp,chan_idx], ev_dur_t1t2[noresp,chan_idx]]\n",
    "fig1, ax1 = plt.subplots()\n",
    "ax1.set_title('Channel-wise diff for resp/non-resp')\n",
    "ax1.boxplot(data);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c91cc3c6",
   "metadata": {},
   "source": [
    "Predict responders/nonresponders by event characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30056f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS CELL DOES A GRID SEARCH\n",
    "chan2try = 'C2'\n",
    "del optimal_params,logRegOutput\n",
    "X = ev_dur_t1t2[:,list(t1_evs_dict.keys()).index(chan2try)]\n",
    "X = np.reshape(X,(-1, 1))\n",
    "y = np.array(resp_or_no)\n",
    "# split into training/testing subsets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=16)\n",
    "\n",
    "# possible settings to try with a grid search over log reg settings\n",
    "param_grid = [\n",
    "    {'penalty' : ['none', 'l1', 'l2', 'elasticnet'],\n",
    "     'C' : np.logspace(-4, 4, 20),\n",
    "     'solver' : ['lbfgs', 'newton-cg', 'liblinear', 'sag', 'saga'],\n",
    "     'max_iter' : [100, 1000, 2500, 5000, 10000]}]\n",
    "\n",
    "# perform grid search\n",
    "optimal_params = GridSearchCV(\n",
    "    LogisticRegression(), \n",
    "    param_grid, \n",
    "    verbose = False,\n",
    "    n_jobs = -1)\n",
    "\n",
    "# fit the model with ideal params\n",
    "optimal_params.fit(X_train, y_train);\n",
    "print('Our optimized parameters:')\n",
    "print(optimal_params.best_params_)\n",
    "logRegOutput = LogisticRegression(C=optimal_params.best_params_['C'], \n",
    "                            penalty=optimal_params.best_params_['penalty'],\n",
    "                            solver = optimal_params.best_params_['solver'],\n",
    "                            max_iter = optimal_params.best_params_['max_iter'],\n",
    "                            random_state = 16)\n",
    "logRegOutput.fit(X_train, y_train)\n",
    "\n",
    "# use the model to predict outcomes in held-out testing subset\n",
    "y_pred = logRegOutput.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adde7897",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rough accuracy score\n",
    "predictionScore = logRegOutput.score(X_test,y_test)\n",
    "print('Prediction: ' + str(round(predictionScore*100)) + \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddddc0ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make confusion matrix to see how off we were for responders, nonresponders\n",
    "cnf_matrix = metrics.confusion_matrix(y_test,y_pred)\n",
    "\n",
    "class_names=[0,1] # name  of classes\n",
    "fig, ax = plt.subplots()\n",
    "tick_marks = np.arange(len(class_names))\n",
    "plt.xticks(tick_marks, class_names)\n",
    "plt.yticks(tick_marks, class_names)\n",
    "# create heatmap\n",
    "sns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap=\"YlGnBu\" ,fmt='g')\n",
    "ax.xaxis.set_label_position(\"top\")\n",
    "plt.tight_layout()\n",
    "plt.title('Confusion matrix', y=1.1)\n",
    "plt.ylabel('Actual label')\n",
    "plt.xlabel('Predicted label')\n",
    "\n",
    "#Text(0.5,257.44,'Predicted label');\n",
    "\n",
    "# get precision, recall, F1\n",
    "target_names = ['non-responder','responder']\n",
    "print(classification_report(y_test, y_pred, target_names=target_names,zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5022419",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AUC score\n",
    "plt.close('all')\n",
    "y_pred_proba = logRegOutput.predict_proba(X_test)[::,1]\n",
    "fpr, tpr, _ = metrics.roc_curve(y_test,  y_pred_proba)\n",
    "auc = metrics.roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "plt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc))\n",
    "plt.legend(loc=4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54686b33",
   "metadata": {},
   "source": [
    "Get beta ev T1-T2 info for the validation sets to make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e5f8ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preallocate\n",
    "ev_dur_val_t1t2 = np.empty((0,62))\n",
    "epochs = read_epochs_eeglab(t1_val_fname[0])\n",
    "pos = epochs.info\n",
    "\n",
    "# cycle through subjects\n",
    "for subj_idx, subj_file in enumerate(evsets_val_t2):\n",
    "    # load evs data\n",
    "    subj_ID = subj_file[-14:-10] # subject ID, they're not in order in the list\n",
    "    series_ID = subj_file[-8:-7]\n",
    "    \n",
    "    t2_evs_dict = []\n",
    "    t1_evs_dict = []\n",
    "    \n",
    "    # load pickles\n",
    "    load_file = open(subj_file,'rb')\n",
    "    t2_evs_dict = pickle.load(load_file)\n",
    "    # find which sub coresponds to t2\n",
    "    index = [idx for idx, s in enumerate(evsets_val_t1) if str(subj_ID+'_S'+series_ID) in s]\n",
    "    load_file = open(evsets_val_t1[index[0]],'rb')\n",
    "    t1_evs_dict = pickle.load(load_file)\n",
    "    \n",
    "    # ev duration\n",
    "    ev_dur_chans = []\n",
    "\n",
    "    for chan_name in t1_evs_dict.keys():\n",
    "        ev_dur_chans.append(100*(avg_event_dur(t2_evs_dict[chan_name])-avg_event_dur(t1_evs_dict[chan_name]))/avg_event_dur(t1_evs_dict[chan_name]))\n",
    "        \n",
    "    ev_dur_val_t1t2 = np.append(ev_dur_val_t1t2, [ev_dur_chans], axis=0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "032c8afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot results - duration\n",
    "plt.close('all')\n",
    "dur_data = np.mean(ev_dur_val_t1t2,axis=0)\n",
    "fig3 = mne.viz.plot_topomap(dur_data, pos,names=list(t1_evs_dict.keys()),show_names=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcdf5d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the model we defined previously to predict outcomes in the entire validation set\n",
    "X_validation = ev_dur_val_t1t2[:,list(t1_evs_dict.keys()).index('C2')]\n",
    "X_validation = np.reshape(X_validation,(-1, 1))\n",
    "y_pred_val = logRegOutput.predict(X_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae36738",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_pred_val,np.mean(y_pred_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08ebf08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cycle through subjects\n",
    "for subj_idx, subj_file in enumerate(evsets_val_t1):\n",
    "    # load evs data\n",
    "    subj_ID = subj_file[-14:-10] \n",
    "    series_ID = subj_file[-8:-7]\n",
    "    subsin1[subj_idx] = subj_ID\n",
    "    seriesin1[subj_idx] = series_ID\n",
    "    \n",
    "    index = [idx for idx, s in enumerate(evsets_val_t2) if str(subj_ID+'_S'+series_ID) in s]\n",
    "    \n",
    "    if not index:\n",
    "        print('Subject '+str(subj_ID)+' series '+str(series_ID)+' does not have a T2 recording.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4667cc1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "# make and save .csv file with subject numbers and predictions\n",
    "with open('validation_predictions.csv','w',newline='') as csv_file:\n",
    "    csv_writer = csv.writer(csv_file)\n",
    "    for subj_idx, subj_file in enumerate(evsets_val_t2):\n",
    "        # load evs data\n",
    "        subj_ID = subj_file[-14:-10] # subject ID, they're not in order in the list\n",
    "        series_ID = subj_file[-8:-7]    \n",
    "\n",
    "        nextrow = [str(subj_ID.lstrip('0')),str(series_ID),str(y_pred_val[subj_idx])]\n",
    "        # add ID and prediction value\n",
    "        csv_writer.writerow(nextrow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f98203f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
